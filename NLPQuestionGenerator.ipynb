{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPQuestionGenerator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1qqD5Zd08LO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "795b6f9a-558d-42bd-b044-71990348d4ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "import nltk.data\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import re\n",
        "import spacy\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "stemmer = LancasterStemmer()"
      ],
      "metadata": {
        "id": "znkLKfW11W25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to hold all input sentences\n",
        "sentences = []\n",
        "\n",
        "# Dictionary to hold sentences corresponding to respective discourse markers\n",
        "disc_sentences = {}\n",
        "\n",
        "# Remaining sentences which do not have discourse markers (To be used later to generate other kinds of questions)\n",
        "nondisc_sentences = []\n",
        "\n",
        "# List of auxiliary verbs\n",
        "aux_list = ['am', 'are', 'is', 'was', 'were', 'can', 'could', 'does', 'do', 'did', 'has', 'had', 'may', 'might', 'must',\n",
        "            'need', 'ought', 'shall', 'should', 'will', 'would']\n",
        "\n",
        "# List of all discourse markers\n",
        "discourse_markers = ['because', 'as a result', 'since', 'when', 'although', 'for example', 'for instance']\n",
        "\n",
        "# Different question types possible for each discourse marker\n",
        "qtype = {'because': ['Why'], 'since': ['When', 'Why'], 'when': ['When'], 'although': ['Yes/No'], 'as a result': ['Why'], \n",
        "        'for example': ['Give an example where'], 'for instance': ['Give an instance where'], 'to': ['Why']}\n",
        "\n",
        "# The argument which forms a question\n",
        "target_arg = {'because': 1, 'since': 1, 'when': 1, 'although': 1, 'as a result': 2, 'for example': 1, 'for instance': 1, \n",
        "              'to': 1}"
      ],
      "metadata": {
        "id": "ERDQmKsk1ZCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentensify():\n",
        "    global sentences\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    fp = open('input.txt')\n",
        "    data = fp.read()\n",
        "    sentences = tokenizer.tokenize(data)\n",
        "    discourse()"
      ],
      "metadata": {
        "id": "zYrXjcQR1e8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_question(question_part, type):\n",
        "\n",
        "    ''' \n",
        "        question_part -> Part of input sentence which forms a question\n",
        "        type-> The type of question (why, where, etc)\n",
        "    '''\n",
        "    # Remove full stop and make first letter lower case\n",
        "    question_part = question_part[0].lower() + question_part[1:]\n",
        "    if(question_part[-1] == '.' or question_part[-1] == ','):\n",
        "        question_part = question_part[:-1]\n",
        "        \n",
        "    # Capitalizing 'i' since 'I' is recognized by parsers appropriately    \n",
        "    for i in range(0, len(question_part)):\n",
        "        if(question_part[i] == 'i'):\n",
        "            if((i == 0 and question_part[i+1] == ' ') or (question_part[i-1] == ' ' and question_part[i+1] == ' ')):\n",
        "                question_part = question_part[:i] + 'I' + question_part[i + 1: ]\n",
        "                \n",
        "    question = \"\"\n",
        "    if(type == 'Give an example where' or type == 'Give an instance where'):\n",
        "        question = type + \" \" + question_part + '?'\n",
        "        return question\n",
        "\n",
        "    aux_verb = False\n",
        "    res = None\n",
        "    \n",
        "    # Find out if auxiliary verb already exists\n",
        "    for i in range(len(aux_list)):\n",
        "        if(aux_list[i] in question_part.split()):\n",
        "            aux_verb = True\n",
        "            pos = i\n",
        "            break\n",
        "\n",
        "    # If auxiliary verb exists\n",
        "    if(aux_verb):\n",
        "        \n",
        "        # Tokeninze the part of the sentence from which the question has to be made\n",
        "        text = nltk.word_tokenize(question_part)\n",
        "        tags = nltk.pos_tag(text)\n",
        "        question_part = \"\"\n",
        "        fP = False\n",
        "        \n",
        "        for word, tag in tags:\n",
        "            if(word in ['I', 'We', 'we']):\n",
        "                question_part += 'you' + \" \"\n",
        "                fP = True\n",
        "                continue\n",
        "            question_part += word + \" \"\n",
        "\n",
        "        # Split across the auxiliary verb and prepend it at the start of question part\n",
        "        question = question_part.split(\" \" + aux_list[pos])\n",
        "        if(fP):\n",
        "             question = [\"were \"] + question\n",
        "        else:\n",
        "            question = [aux_list[pos] + \" \"] + question\n",
        "\n",
        "        # If Yes/No, no need to introduce question phrase\n",
        "        if(type == 'Yes/No'):\n",
        "            question += ['?']\n",
        "            \n",
        "        elif(type != \"non_disc\"):\n",
        "            question = [type + \" \"] + question + [\"?\"]\n",
        "            \n",
        "        else:\n",
        "            question = question + [\"?\"]\n",
        "         \n",
        "        question = ''.join(question)\n",
        "\n",
        "    # If auxilary verb does ot exist, it can only be some form of verb 'do'\n",
        "    else:\n",
        "        aux = None\n",
        "        text = nltk.word_tokenize(question_part)\n",
        "        tags = nltk.pos_tag(text)\n",
        "        comb = \"\"\n",
        "\n",
        "        '''There can be following combinations of nouns and verbs:\n",
        "            NN/NNP and VBZ  -> Does\n",
        "            NNS/NNPS(plural) and VBP -> Do\n",
        "            NN/NNP and VBN -> Did\n",
        "            NNS/NNPS(plural) and VBN -> Did\n",
        "        '''\n",
        "        \n",
        "        for tag in tags:\n",
        "            if(comb == \"\"):\n",
        "                if(tag[1] == 'NN' or tag[1] == 'NNP'):\n",
        "                    comb = 'NN'\n",
        "\n",
        "                elif(tag[1] == 'NNS' or tag[1] == 'NNPS'):\n",
        "                    comb = 'NNS'\n",
        "\n",
        "                elif(tag[1] == 'PRP'):\n",
        "                    if tag[0] in ['He','She','It']:\n",
        "                        comb = 'PRPS'\n",
        "                    else:\n",
        "                        comb = 'PRPP'\n",
        "                        tmp = question_part.split(\" \")\n",
        "                        tmp = tmp[1: ]\n",
        "                        if(tag[0] in ['I', 'we', 'We']):\n",
        "                            question_part = 'you ' + ' '.join(tmp)\n",
        "                            \n",
        "            if(res == None):\n",
        "                res = re.match(r\"VB*\", tag[1])\n",
        "                if(res):\n",
        "                    \n",
        "                    # Stem the verb\n",
        "                    question_part = question_part.replace(tag[0], stemmer.stem(tag[0]))\n",
        "                res = re.match(r\"VBN\", tag[1])\n",
        "                res = re.match(r\"VBD\", tag[1])\n",
        "\n",
        "        if(comb == 'NN'):\n",
        "            aux = 'does'\n",
        "            \n",
        "        elif(comb == 'NNS'):\n",
        "            aux = 'do'\n",
        "            \n",
        "        elif(comb == 'PRPS'):\n",
        "            aux = 'does'\n",
        "            \n",
        "        elif(comb == 'PRPP'):\n",
        "            aux = 'do'\n",
        "            \n",
        "        if(res and res.group() in ['VBD', 'VBN']):\n",
        "            aux = 'did'\n",
        "\n",
        "        if(aux):\n",
        "            if(type == \"non_disc\" or type == \"Yes/No\"):\n",
        "                question = aux + \" \" + question_part + \"?\"\n",
        "\n",
        "            else:\n",
        "                question = type + \" \" + aux + \" \" + question_part + \"?\"\n",
        "    if(question != \"\"):\n",
        "        question = question[0].upper() + question[1:]\n",
        "    return question"
      ],
      "metadata": {
        "id": "YiXkIPVL1gm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function is used to get the named entities\n",
        "def get_named_entities(sent):\n",
        "    doc = nlp(sent)\n",
        "    named_entities = [(X.text, X.label_) for X in doc.ents]\n",
        "    return named_entities"
      ],
      "metadata": {
        "id": "oRwUFfjb1jPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function is used to get the required wh word\n",
        "def get_wh_word(entity, sent):\n",
        "    wh_word = \"\"\n",
        "    if entity[1] in ['TIME', 'DATE']:\n",
        "        wh_word = 'When'\n",
        "        \n",
        "    elif entity[1] == ['PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE']:\n",
        "        wh_word = 'What'\n",
        "        \n",
        "    elif entity[1] in ['PERSON']:\n",
        "            wh_word = 'Who'\n",
        "            \n",
        "    elif entity[1] in ['NORP', 'FAC' ,'ORG', 'GPE', 'LOC']:\n",
        "        index = sent.find(entity[0])\n",
        "        if index == 0:\n",
        "            wh_word = \"Who\"\n",
        "            \n",
        "        else:\n",
        "            wh_word = \"Where\"\n",
        "            \n",
        "    else:\n",
        "        wh_word = \"Where\"\n",
        "    return wh_word"
      ],
      "metadata": {
        "id": "fkuZ5NWF1m__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function generate questions based on NER templates\n",
        "def generate_one_word_questions(sent):\n",
        "    \n",
        "    named_entities = get_named_entities(sent)\n",
        "    questions = []\n",
        "    \n",
        "    if not named_entities:\n",
        "        return questions\n",
        "    \n",
        "    for entity in named_entities:\n",
        "        wh_word = get_wh_word(entity, sent)\n",
        "        \n",
        "        if(sent[-1] == '.'):\n",
        "            sent = sent[:-1]\n",
        "        \n",
        "        if sent.find(entity[0]) == 0:\n",
        "            questions.append(sent.replace(entity[0],wh_word) + '?')\n",
        "            continue\n",
        "       \n",
        "        question = \"\"\n",
        "        aux_verb = False\n",
        "        res = None\n",
        "\n",
        "        for i in range(len(aux_list)):\n",
        "            if(aux_list[i] in sent.split()):\n",
        "                aux_verb = True\n",
        "                pos = i\n",
        "                break\n",
        "            \n",
        "        if not aux_verb:\n",
        "            pos = 9\n",
        "        \n",
        "        text = nltk.word_tokenize(sent)\n",
        "        tags = nltk.pos_tag(text)\n",
        "        question_part = \"\"\n",
        "        \n",
        "        if wh_word == 'When':\n",
        "            word_list = sent.split(entity[0])[0].split()\n",
        "            if word_list[-1] in ['in', 'at', 'on']:\n",
        "                question_part = \" \".join(word_list[:-1])\n",
        "            else:\n",
        "                question_part = \" \".join(word_list)\n",
        "            \n",
        "            qp_text = nltk.word_tokenize(question_part)\n",
        "            qp_tags = nltk.pos_tag(qp_text)\n",
        "            \n",
        "            question_part = \"\"\n",
        "            \n",
        "            for i, grp in enumerate(qp_tags):\n",
        "                word = grp[0]\n",
        "                tag = grp[1]\n",
        "                if(re.match(\"VB*\", tag) and word not in aux_list):\n",
        "                    question_part += WordNetLemmatizer().lemmatize(word,'v') + \" \"\n",
        "                else:\n",
        "                    question_part += word + \" \"\n",
        "                \n",
        "            if question_part[-1] == ' ':\n",
        "                question_part = question_part[:-1]\n",
        "        \n",
        "        else:\n",
        "            for i, grp in enumerate(tags):\n",
        "                \n",
        "                #Break the sentence after the first non-auxiliary verb\n",
        "                word = grp[0]\n",
        "                tag = grp[1]\n",
        "\n",
        "                if(re.match(\"VB*\", tag) and word not in aux_list):\n",
        "                    question_part += word\n",
        "\n",
        "                    if i<len(tags) and 'NN' not in tags[i+1][1] and wh_word != 'When':\n",
        "                        question_part += \" \"+ tags[i+1][0]\n",
        "\n",
        "                    break\n",
        "                question_part += word + \" \"\n",
        "        question = question_part.split(\" \"+ aux_list[pos])\n",
        "        question = [aux_list[pos] + \" \"] + question\n",
        "        question = [wh_word+ \" \"] + question + [\"?\"]\n",
        "        question = ''.join(question)\n",
        "        questions.append(question)\n",
        "    \n",
        "    return questions        "
      ],
      "metadata": {
        "id": "l2j3oF7o1pDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function used to pre-process sentences which have discourse markers in them\n",
        "def discourse():\n",
        "    temp = []\n",
        "    target = \"\"\n",
        "    questions = []\n",
        "    global disc_sentences\n",
        "    disc_sentences = {}\n",
        "    for i in range(len(sentences)):\n",
        "        maxLen = 9999999\n",
        "        val = -1\n",
        "        for j in discourse_markers:\n",
        "            tmp = len(sentences[i].split(j)[0].split(' '))  \n",
        "            \n",
        "            # To get valid, first discourse marker.   \n",
        "            if(len(sentences[i].split(j)) > 1 and tmp >= 3 and tmp < maxLen):\n",
        "                maxLen = tmp\n",
        "                val = j\n",
        "                \n",
        "        if(val != -1):\n",
        "\n",
        "            # To initialize a list for every new key\n",
        "            if(disc_sentences.get(val, 'empty') == 'empty'):\n",
        "                disc_sentences[val] = []\n",
        "                \n",
        "            disc_sentences[val].append(sentences[i])\n",
        "            temp.append(sentences[i])\n",
        "\n",
        "\n",
        "    nondisc_sentences = list(set(sentences) - set(temp))\n",
        "    \n",
        "    t = []\n",
        "    for k, v in disc_sentences.items():\n",
        "        for val in range(len(v)):\n",
        "            \n",
        "            # Split the sentence on discourse marker and identify the question part\n",
        "            question_part = disc_sentences[k][val].split(k)[target_arg[k] - 1]\n",
        "            q = generate_question(question_part, qtype[k][0])\n",
        "            if(q != \"\"):\n",
        "                questions.append([disc_sentences[k][val],q])\n",
        "                \n",
        "                \n",
        "    for question_part in nondisc_sentences:\n",
        "        s = \"non_disc\"\n",
        "        sentence = question_part\n",
        "        text = nltk.word_tokenize(question_part)\n",
        "        if(text[0] == 'Yes'):\n",
        "            question_part = question_part[5:]\n",
        "            s = \"Yes/No\"\n",
        "            \n",
        "        elif(text[0] == 'No'):\n",
        "            question_part = question_part[4:]\n",
        "            s = \"Yes/No\"\n",
        "            \n",
        "        q = generate_question(question_part, s)\n",
        "        if(q != \"\"):\n",
        "            questions.append([sentence,q])\n",
        "        l = generate_one_word_questions(question_part)\n",
        "        questions += [[sentence,i] for i in l]\n",
        "    print(len(questions))\n",
        "    \n",
        "    for pair in questions:\n",
        "        print(\"S: \",pair[0])\n",
        "        print(\"Q: \",pair[1])\n",
        "        print()"
      ],
      "metadata": {
        "id": "Om5qzrAP1q9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syntactic_score = [0,0,1,1,1,1,1,0,1,1,1,0,1,0,1,1,1,1,1,1,1,0,1,1,0,1,0,1,1,1,1,1,0,1,1,1,1,0,1,0,1,0,1,1,1,1,1,1,0,0,\n",
        "                    1,1,1,0,1,1,0,1,1,1,1,1,1,1,1,0,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,0,1,1,1,0,1,1,1,0,1,1]\n",
        "fluency_score   = [0,0,1,1,1,0,1,0,1,1,1,0,1,0,1,1,1,1,1,1,1,0,0,0,0,1,0,1,1,1,1,1,0,0,1,1,1,0,0,0,0,1,1,1,1,1,1,0,0,0,\n",
        "                    1,1,1,0,1,1,0,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,0,1,1,0,1,1,1,0,1,1,1,0,0,0,0,0,1,1]\n",
        "print(len(syntactic_score))\n",
        "print(sum(syntactic_score)/len(syntactic_score))\n",
        "print(sum(fluency_score)/sum(syntactic_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_X5uGJ31tMI",
        "outputId": "7f3968bc-c329-4dff-e450-34faf9ccee96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92\n",
            "0.7608695652173914\n",
            "0.8428571428571429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentensify()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "uGlFp2Ro1voo",
        "outputId": "13bb8f25-2855-472a-98a9-3c15a9e5ecaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-04598b6156be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentensify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-a2dffe3357db>\u001b[0m in \u001b[0;36msentensify\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/english.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ps6hcRaI1xgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}